\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{float}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}[theorem]{Proposition}

% Custom commands
\newcommand{\manifold}{\mathcal{M}}
\newcommand{\graph}{\mathcal{G}}
\newcommand{\vectorspace}{\mathcal{V}}
\newcommand{\embedding}{\phi}
\newcommand{\projection}{\psi}

\title{Context Manifold: Adaptive Hybrid Graph-Vector Retrieval for Code Intelligence}

\author{
John Schmotzer\\
Independent Researcher\\
\texttt{schmotzer.john@gmail.com}
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
Retrieval-augmented generation (RAG) for code intelligence is commonly implemented as vector similarity search over embedded code chunks. However, software repositories also contain explicit structural relationships (e.g., function calls and type references) that can be exploited for context selection. We present \textbf{Context Manifold}, a hybrid retrieval formulation that combines normalized graph distance and normalized embedding distance in a single metric: $d_M=\lambda \hat d_G+(1-\lambda)\hat d_V$.

We evaluate Context Manifold on 70 real-world Python repositories (44{,}488 function-level queries) using \textit{dependency coverage} against a static-analysis ground truth call graph. Aggregate performance is near-neutral (mean DC 0.616 for a vector baseline vs. 0.613 for the hybrid), but results are strongly heterogeneous by repository structure. Repositories with higher internal coupling benefit from graph expansion (+4.1\% mean $\Delta$DC across the manifold-suitable cohort), while low-coupling utility libraries regress (-5.2\%), indicating that hybrid retrieval should be applied selectively.

These results motivate an adaptive routing strategy: compute lightweight structural diagnostics during indexing (e.g., edge density) and choose between vector-only and hybrid retrieval per codebase (or per query). We report limitations and provide implementation details to support replication and extension.
\end{abstract}

\section{Introduction}

\subsection{The Context Crisis}

Large language models have revolutionized software development, document processing, and knowledge work. However, these systems face a fundamental limitation: \textit{finite context windows cannot capture the infinite complexity of real-world systems}. When an LLM attempts to reason about a codebase with thousands of files, a product specification with hundreds of requirements, or a legal corpus spanning decades, it must compress, truncate, or selectively retrieve information---each approach introducing systematic errors.

These errors manifest as hallucinations: the model invents function signatures that do not exist, references requirements that were truncated, or makes logical leaps across information gaps it cannot perceive. The hallucination is not a failure of the model's reasoning---it is a rational response to incomplete information presented as complete.

Recent benchmarks quantify this problem:
\begin{itemize}
    \item CoderEval~\citep{codereval2023} demonstrates dramatic performance degradation on repository-level code generation tasks
    \item RAGTruth corpus~\citep{ragtruth2024} documents 15--25\% hallucination rates even with retrieval augmentation
    \item HalluCode~\citep{hallucode2024} identifies ``undefined reference'' and ``API misuse'' as comprising over 40\% of code generation errors---directly attributable to incomplete context
\end{itemize}

\subsection{Current Approaches and Limitations}

\textbf{Extended Context Windows.} Models like Claude (200K tokens) and Gemini (1M tokens) expand capacity but face $O(n^2)$ attention complexity. More fundamentally, KG-LM benchmarks~\citep{kglm2024} show accuracy degrades to 0\% on schema-bound queries as entity count exceeds 5, regardless of window size. Longer windows do not solve structural blindness.

\textbf{Vector RAG.} Retrieval-augmented generation using semantic similarity enables relevant chunk retrieval but loses structural relationships. Lettria benchmarks demonstrate 0\% accuracy on multi-hop queries with pure vector retrieval. FalkorDB reproductions show 16\% accuracy versus 54\% when knowledge graphs are incorporated.

\textbf{GraphRAG.} Knowledge graph-enhanced retrieval improves structural awareness but requires expensive graph construction. MLOps analyses show improved faithfulness but similar performance on other RAGAS metrics, suggesting incomplete integration of graph and vector modalities.

Each approach treats context as a \textit{flat sequence}---a linear array of tokens to be searched, compressed, or extended. This fundamentally misrepresents the structure of real-world knowledge, which is inherently \textit{relational, hierarchical, and multi-dimensional}.

\subsection{The Context Manifold Thesis}

We propose that context should be modeled not as a sequence but as a \textbf{manifold}---a topological space that is locally Euclidean (supporting vector operations) but globally non-linear (capturing complex relationships). In this formulation:

\begin{itemize}
    \item \textbf{Knowledge graphs} provide the manifold's topological structure, encoding entities as nodes and relationships as edges
    \item \textbf{Vector embeddings} provide local coordinate charts, enabling semantic similarity operations within neighborhoods
    \item \textbf{Graph traversal} acts as geodesic navigation, finding shortest paths through concept space
    \item \textbf{Embedded vector references} serve as coordinates linking discrete graph structure to continuous embedding space
\end{itemize}

This hybrid structure---which we term the \textbf{Context Manifold}---enables infinite scalability: rather than loading entire contexts, AI systems navigate the manifold, retrieving precisely the subgraph and associated embeddings needed for each query while maintaining awareness of the broader topological structure.

\section{Formal Definition}

\subsection{Mathematical Framework}

\begin{definition}[Context Manifold]
A Context Manifold is a tuple $\manifold = (\graph, \vectorspace, \embedding, \projection, \Gamma)$ where:
\begin{itemize}
    \item $\graph = (N, E, \tau_N, \tau_E)$ is a typed knowledge graph with nodes $N$, edges $E$, and type functions $\tau_N: N \to T_N$, $\tau_E: E \to T_E$
    \item $\vectorspace \subseteq \mathbb{R}^d$ is a $d$-dimensional embedding space
    \item $\embedding: N \to \vectorspace$ maps nodes to embedding vectors
    \item $\projection: \vectorspace \to 2^N$ maps query vectors to relevant node sets
    \item $\Gamma: N \times N \to \mathbb{R}^+$ defines geodesic distances combining graph and embedding metrics
\end{itemize}
\end{definition}

\subsection{Topological Structure}

The manifold exhibits dual locality:

\textbf{Local neighborhoods} (embedding space): For node $n$ and radius $\epsilon$, the $\epsilon$-ball $B_\epsilon(n) = \{m \in N : \|\embedding(n) - \embedding(m)\| < \epsilon\}$ captures semantically similar nodes.

\textbf{Global structure} (graph space): For node $n$ and distance $k$, the $k$-hop neighborhood $N_k(n) = \{m \in N : d_\graph(n,m) \leq k\}$ captures structurally related nodes.

\textbf{Manifold geodesic}: The combined metric balances both:
\begin{equation}
\Gamma(n, m) = \alpha \cdot d_\graph(n, m) + (1 - \alpha) \cdot \|\embedding(n) - \embedding(m)\|
\end{equation}
where $\alpha \in [0,1]$ controls the tradeoff between structural and semantic distance.

\subsection{Information-Theoretic Foundation}

We define context quality through information preservation:

\begin{definition}[Context Entropy]
For query $q$ and context $C$, context entropy is:
\begin{equation}
H(C|q) = -\sum_{c \in C} P(c|q) \log P(c|q)
\end{equation}
\end{definition}

\begin{definition}[Structural Mutual Information]
The mutual information between retrieved context $C$ and ground-truth dependencies $D$ is:
\begin{equation}
I(C; D) = H(D) - H(D|C)
\end{equation}
\end{definition}

\begin{theorem}[Manifold Information Preservation]
For a well-constructed Context Manifold $\manifold$ with appropriate $\alpha$, the manifold-based retrieval preserves strictly more dependency information than vector-only retrieval:
\begin{equation}
I(C_\manifold; D) > I(C_{\text{vector}}; D)
\end{equation}
\end{theorem}

\begin{proof}[Proof sketch]
Vector-only retrieval captures $I_{\text{semantic}}$ but loses $I_{\text{structural}}$ from edge relationships. The manifold retrieval captures both through the combined geodesic, with graph traversal recovering structural dependencies invisible to embedding similarity alone. The inequality holds when $\alpha > 0$ and the knowledge graph encodes non-trivial structural relationships.
\end{proof}

\section{Architecture}

\subsection{Component Stack}

The Context Manifold implementation requires four integrated components:

\begin{enumerate}
    \item \textbf{Graph Database} (e.g., FalkorDB, Neo4j): Stores nodes, edges, and relationship types with efficient traversal
    \item \textbf{Vector Database} (e.g., Pinecone, Weaviate, Qdrant): Stores embeddings with approximate nearest neighbor search
    \item \textbf{Document Store} (e.g., MongoDB, S3): Stores raw content chunks referenced by nodes
    \item \textbf{Embedding Service}: Generates vectors for nodes and queries
\end{enumerate}

\subsection{The Embedded Reference Pattern}

The critical architectural innovation is \textbf{bidirectional linking} between graph nodes and vector embeddings:

\begin{verbatim}
GraphNode {
  id: "func_auth_user",
  type: "function",
  properties: { name: "authenticate_user", file: "auth.py" },
  vector_id: "emb_7f3a2b1c",  // Reference to vector DB
  content_id: "doc_auth_42"    // Reference to document store
}

VectorRecord {
  id: "emb_7f3a2b1c",
  vector: [0.23, -0.41, ...],  // 1536-dim embedding
  metadata: { graph_id: "func_auth_user" }  // Back-reference
}
\end{verbatim}

This pattern enables:
\begin{itemize}
    \item Vector similarity search $\to$ graph node $\to$ relationship traversal
    \item Graph traversal $\to$ node embeddings $\to$ semantic expansion
    \item Seamless interleaving of both retrieval modalities
\end{itemize}

\subsection{Retrieval Algorithm}

\begin{algorithm}
\caption{Context Manifold Retrieval}
\begin{algorithmic}[1]
\Require Query $q$, manifold $\manifold$, expansion depth $k$, semantic radius $\epsilon$
\Ensure Context set $C$
\State $v_q \gets \text{embed}(q)$ \Comment{Embed query}
\State $N_{\text{seed}} \gets \projection(v_q, \epsilon)$ \Comment{Vector similarity seeds}
\State $N_{\text{expanded}} \gets \emptyset$
\For{$n \in N_{\text{seed}}$}
    \State $N_{\text{expanded}} \gets N_{\text{expanded}} \cup N_k(n)$ \Comment{$k$-hop graph expansion}
\EndFor
\State $N_{\text{ranked}} \gets \text{sort}(N_{\text{expanded}}, \lambda m: \Gamma(v_q, m))$ \Comment{Rank by geodesic}
\State $C \gets \text{fetch\_content}(N_{\text{ranked}}[:\text{limit}])$
\State \Return $C$
\end{algorithmic}
\end{algorithm}

\section{Reference Implementations}

\subsection{cv-git: Code Intelligence}

The cv-git system\footnote{\url{https://github.com/controlVector/cv-git}} implements the Context Manifold for software repositories:

\textbf{Node types}: Files, functions, classes, modules, tests, commits, authors

\textbf{Edge types}: \texttt{calls}, \texttt{imports}, \texttt{inherits}, \texttt{tests}, \texttt{authored\_by}, \texttt{depends\_on}

\textbf{Use case}: When an AI agent needs to modify \texttt{authenticate\_user()}, the manifold retrieval:
\begin{enumerate}
    \item Finds semantically similar functions (vector search)
    \item Traverses to callers, callees, and test functions (graph expansion)
    \item Retrieves type definitions for parameters and return values
    \item Provides complete context without loading entire codebase
\end{enumerate}

\subsection{cv-prd: Requirements Intelligence}

The cv-prd system\footnote{\url{https://github.com/controlVector/cv-prd}} applies the manifold to product requirements:

\textbf{Node types}: Requirements, features, user stories, constraints, stakeholders

\textbf{Edge types}: \texttt{implements}, \texttt{depends\_on}, \texttt{conflicts\_with}, \texttt{owned\_by}

\textbf{Use case}: When generating a technical specification, the manifold ensures all dependent requirements, constraints, and stakeholder concerns are included in context.

\section{Metrics for Context Preservation}

\subsection{Primary Metrics}

\begin{definition}[Dependency Coverage (DC)]
\begin{equation}
DC = \frac{|C_{\text{retrieved}} \cap D_{\text{ground\_truth}}|}{|D_{\text{ground\_truth}}|}
\end{equation}
where $D_{\text{ground\_truth}}$ is determined by static analysis (for code) or expert annotation.
\end{definition}

\begin{definition}[Relative Hallucination Rate (RHR)]
\begin{equation}
RHR = \frac{\text{Claims unsupported by context}}{\text{Total claims in output}}
\end{equation}
\end{definition}

\begin{definition}[Context Efficiency (CE)]
\begin{equation}
CE = \frac{\text{Tokens contributing to correct output}}{\text{Total tokens in context}}
\end{equation}
\end{definition}

\subsection{Complexity Scaling Hypothesis}

We hypothesize that manifold advantage scales with codebase complexity:

\begin{definition}[Complexity Index]
\begin{equation}
CI = \log(|N|) \times \bar{d} \times \bar{p}
\end{equation}
where $|N|$ is node count, $\bar{d}$ is average degree, and $\bar{p}$ is average path length.
\end{definition}

\textbf{Hypothesis}: The performance improvement ratio follows:
\begin{equation}
R(CI) = 1 + \beta \cdot CI^\gamma
\end{equation}
where $\beta$ and $\gamma$ are empirically determined constants. This predicts that for simple codebases, vector RAG may suffice, but costs grow super-linearly with complexity, making the Context Manifold essential for enterprise systems.

\section{Economic Model}

\subsection{Hallucination Cost Analysis}

Based on industry surveys and developer time studies:

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Error Type} & \textbf{Frequency} & \textbf{Debug Time} & \textbf{Cost @ \$100/hr} \\
\midrule
Undefined reference & 35\% & 0.5 hr & \$50 \\
API signature mismatch & 25\% & 0.75 hr & \$75 \\
Type error & 20\% & 0.25 hr & \$25 \\
Logic error (context gap) & 15\% & 1.5 hr & \$150 \\
Other & 5\% & 0.5 hr & \$50 \\
\midrule
\textbf{Weighted average} & & & \textbf{\$62.50} \\
\bottomrule
\end{tabular}
\caption{Weighted hallucination cost by error type}
\end{table}

\subsection{Infrastructure Cost Comparison}

\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Component} & \textbf{Vector RAG} & \textbf{Context Manifold} \\
\midrule
Vector DB (managed) & \$500/mo & \$500/mo \\
Graph DB (managed) & --- & \$300/mo \\
Document store & \$100/mo & \$150/mo \\
Embedding compute & \$100/mo & \$100/mo \\
Additional indexing & \$10/mo & \$20/mo \\
\midrule
\textbf{Total} & \$710/mo & \$1,070/mo \\
\textbf{Incremental cost} & --- & \$360/mo \\
\bottomrule
\end{tabular}
\caption{Monthly infrastructure costs (mid-tier deployment)}
\end{table}

\subsection{ROI Calculation}

For an enterprise team generating 1,000 AI-assisted code generations per month:

\textbf{Vector RAG baseline}:
\begin{itemize}
    \item Hallucination rate: 25\%
    \item Hallucinations/month: 250
    \item Monthly cost: 250 $\times$ \$62.50 = \$15,625
\end{itemize}

\textbf{Context Manifold}:
\begin{itemize}
    \item Hallucination rate: 10\% (60\% reduction)
    \item Hallucinations/month: 100
    \item Monthly cost: 100 $\times$ \$62.50 = \$6,250
\end{itemize}

\textbf{Net benefit}: \$15,625 - \$6,250 - \$360 = \textbf{\$9,015/month}

\textbf{ROI}: \$9,015 / \$360 = \textbf{25$\times$}

\section{Experimental Protocol}

\subsection{Dataset Construction}

\textbf{Repository selection}: 50 Python repositories from GitHub satisfying:
\begin{itemize}
    \item 1,000--50,000 lines of code
    \item Test coverage $>$ 60\%
    \item Active development (commits within 6 months)
    \item Diverse domains (web, data science, CLI tools, libraries)
\end{itemize}

\textbf{Task construction}: For each repository, select 10 functions meeting:
\begin{itemize}
    \item Calls $\geq 2$ other repository-internal functions
    \item Uses $\geq 1$ custom type or class
    \item Has associated test coverage
\end{itemize}

Total: 500 function completion tasks.

\subsection{Conditions}

\textbf{Condition A (Vector RAG baseline)}:
\begin{itemize}
    \item Chunk repository into 512-token segments
    \item Embed with text-embedding-3-large
    \item Retrieve top-20 chunks by cosine similarity
\end{itemize}

\textbf{Condition B (Context Manifold)}:
\begin{itemize}
    \item Same vector index as Condition A
    \item Add FalkorDB graph with function/class/import relationships
    \item Retrieve top-10 by similarity + 2-hop graph expansion
\end{itemize}

\subsection{Ground Truth}

Static analysis extracts actual dependencies:
\begin{itemize}
    \item Function calls (AST traversal)
    \item Type references (type annotations + inference)
    \item Import dependencies (module resolution)
\end{itemize}

\subsection{Results Summary}

A detailed summary of the validation findings is provided in Section~\ref{sec:empirical-results}, including full tables and an ED--$\\Delta$DC plot. In brief: the overall mean effect is near-neutral, but cohort-level heterogeneity is substantial (enterprise systems improve; utility libraries regress), motivating an adaptive routing policy for when to enable graph expansion.\section{Empirical Results}\label{sec:empirical-results}

This section integrates the validation study results (70 repositories, 44{,}488 retrieval tasks) into the manuscript and clarifies the operational definition of edge density used throughout.

\subsection{Datasets and Graph Statistics}

Table~\ref{tab:dataset} summarizes the repository cohorts used for evaluation. Table~\ref{tab:graph-stats} reports aggregate graph statistics across the evaluation corpus. We define \textbf{edge density} as
\begin{equation}
\mathrm{ED} \;=\; \frac{|E_{\mathrm{calls}}|}{|V_{\mathrm{fn}}|},
\end{equation}
i.e., the number of static \textsc{calls} edges per function node. For the full corpus, this yields $\mathrm{ED}\approx 14.85$.

\begin{table}
\caption{Dataset composition used in validation.}
\label{tab:dataset}
\begin{tabular}{p{0.20\textwidth}r r p{0.45\textwidth}}
\toprule
Category & Repositories & Tasks & Description \\
\midrule
Enterprise Systems & 10 & 8,847 & Airflow, Celery, Prefect, Dagster, MLflow \\
Large Frameworks & 10 & 7,234 & Django, FastAPI, Flask, Starlette, Sanic \\
Web Extensions & 10 & 4,222 & Flask plugins, authentication libraries \\
Data Processing & 10 & 8,322 & Data manipulation and ETL tools \\
ML/AI Tools & 10 & 4,771 & Machine learning utilities \\
Libraries/Utilities & 10 & 5,489 & General-purpose helper libraries \\
DevOps/CLI & 10 & 4,073 & Command-line tools \\
Total & 70 & 44,488 &  \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Graph and corpus statistics.}
\label{tab:graph-stats}
\begin{tabular}{p{0.35\textwidth}r}
\toprule
Metric & Value \\
\midrule
Function Nodes & 138,815 \\
CALLS Edges & 2,061,755 \\
Average Edge Density & 14.9 \\
ChromaDB Documents & 47,583 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Overall Effect and Heterogeneity}

Table~\ref{tab:overall-dc} shows overall dependency coverage (DC) across all tasks. While the mean difference is small in absolute terms, stratifying by repository type reveals substantial heterogeneity (Table~\ref{tab:by-class}): complex, highly-interdependent systems benefit from manifold expansion, while utility-style libraries regress.

\begin{table}
\caption{Overall dependency coverage (DC) across all tasks.}
\label{tab:overall-dc}
\begin{tabular}{p{0.35\textwidth}c c r}
\toprule
Condition & Mean DC & Std Dev & N \\
\midrule
Vector (Baseline) & 0.616 & 0.312 & 44,488 \\
Manifold (Treatment) & 0.613 & 0.318 & 44,488 \\
Difference & -0.002 & 0.198 &  \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Results stratified by observed effect (classification based on $\Delta$DC).}
\label{tab:by-class}
\begin{tabular}{p{0.30\textwidth}c c p{0.30\textwidth}}
\toprule
Classification & N Repos & Mean $\Delta$ DC & Interpretation \\
\midrule
Manifold-Suitable & 21 (30\%) & +4.1\% & Graph expansion helps \\
Neutral & 29 (41\%) & -0.5\% & Either approach works \\
Vector-Sufficient & 20 (29\%) & -5.2\% & Vector-only optimal \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Representative Repositories and Platforms}

Table~\ref{tab:top-manifold} lists representative repositories with the largest observed improvements. Table~\ref{tab:top-vector} lists repositories where graph expansion consistently hurts performance. Table~\ref{tab:platforms} summarizes enterprise-scale frameworks.

\textbf{Important nuance:} ED is strongly associated with the \emph{cohort-level} benefit of manifold retrieval (Table~\ref{tab:structural}), but it is not a perfect per-repository classifier: some web frameworks show positive gains despite low call-edge density, suggesting that dependency breadth (e.g., imports, types, and cross-module coupling) can dominate pure call-graph density for certain tasks. Consequently, the ED-based routing rule in Table~\ref{tab:selection} should be treated as a practical heuristic rather than a guarantee.

\begin{longtable}{p{0.30\textwidth}p{0.25\textwidth}c c}
\caption{Top manifold-suitable repositories (largest positive $\Delta$DC).} \label{tab:top-manifold} \\
\toprule
Repository & Category & $\Delta$ DC & Edge Density \\
\midrule
\endfirsthead
\caption[]{Top manifold-suitable repositories (largest positive $\Delta$DC).} \\
\toprule
Repository & Category & $\Delta$ DC & Edge Density \\
\midrule
\endhead
\midrule
\multicolumn{4}{r}{Continued on next page} \\
\midrule
\endfoot
\bottomrule
\endlastfoot
flask-restful & Web Framework & +16.5\% & 0.80 \\
flask-restx & Web Framework & +14.5\% & 2.34 \\
flask-jwt-extended & Web Framework & +9.1\% & 0.72 \\
imbalanced-learn & ML Framework & +8.1\% & 2.97 \\
flask-wtf & Web Framework & +7.4\% & 0.68 \\
werkzeug & HTTP Toolkit & +5.8\% & 2.62 \\
petl & Data Processing & +4.9\% & 1.95 \\
pandas-datareader & Data Processing & +4.7\% & 4.05 \\
mlxtend & ML Framework & +4.2\% & 0.74 \\
authlib & Auth Framework & +3.5\% & 3.15 \\
FastAPI & Web Framework & +1.8\% & 5.38 \\
Apache Airflow & Enterprise & +0.6\% & 60.89 \\
\end{longtable}


\begin{table}
\caption{Vector-sufficient repositories (largest negative $\Delta$DC).}
\label{tab:top-vector}
\begin{tabular}{p{0.30\textwidth}p{0.25\textwidth}c c}
\toprule
Repository & Category & $\Delta$ DC & Edge Density \\
\midrule
missingno & Visualization & -25.0\% & 0.88 \\
pandarallel & Parallel Utils & -12.0\% & 0.46 \\
docopt & CLI Parser & -10.1\% & 0.56 \\
toolz & Functional Utils & -9.6\% & 0.58 \\
arrow & Date Utils & -6.4\% & 0.59 \\
kedro & ML Pipeline & -5.8\% & 1.41 \\
more-itertools & Itertools & -5.4\% & 0.45 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Enterprise platform results.}
\label{tab:platforms}
\begin{tabular}{p{0.30\textwidth}p{0.32\textwidth}c c}
\toprule
Platform & Type & $\Delta$ DC & Edge Density \\
\midrule
Apache Airflow & Workflow Orchestration & +0.6\% & 60.89 \\
FastAPI & Web Framework & +1.8\% & 5.38 \\
Prefect & Workflow Orchestration & 0.0\% & 4.60 \\
Dagster & Data Orchestration & 0.0\% & 10.02 \\
Django & Web Framework & -0.2\% & 14.39 \\
MLflow & ML Platform & -0.2\% & 12.94 \\
Celery & Task Queue & -0.4\% & 5.47 \\
Luigi & Workflow & -0.4\% & 4.88 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Structural Differentiators and Operational Heuristic}

Table~\ref{tab:structural} quantifies structural differences between cohorts. We provide an operational routing heuristic in Table~\ref{tab:selection}, intended for production systems to decide when to enable graph expansion.

\begin{table}
\caption{Structural differences between cohorts.}
\label{tab:structural}
\begin{tabular}{p{0.28\textwidth}c c c}
\toprule
Metric & Manifold-Suitable & Vector-Sufficient & Ratio \\
\midrule
Edge Density & 4.72 & 1.52 & 3.1x \\
Avg Dependencies & 29.3 & 4.1 & 7.1x \\
Connectivity & 67.9\% & 65.7\% & 1.03x \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Operational heuristic for selecting retrieval strategy based on edge density.}
\label{tab:selection}
\begin{tabular}{c p{0.28\textwidth}p{0.28\textwidth}p{0.25\textwidth}}
\toprule
Edge Density & Classification & Strategy & Expected Result \\
\midrule
> 3.0 & MANIFOLD-SUITABLE & Graph + Vector Hybrid & +4.1\% DC improvement \\
1.5 - 3.0 & NEUTRAL & Either approach & ~0\% difference \\
< 1.5 & VECTOR-SUFFICIENT & Pure Vector Search & Optimal performance \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Edge Density vs. Observed Benefit}

Figure~\ref{fig:ed-vs-delta} plots $\\Delta$DC against ED for representative repositories and platforms. The vertical markers indicate the heuristic breakpoints from Table~\ref{tab:selection}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.78\textwidth]{edge_density_vs_delta.pdf}
    \caption{Observed change in dependency coverage ($\\Delta$DC) versus edge density (CALLS edges per function) for representative repositories and platforms. The two dashed lines denote ED = 1.5 and ED = 3.0.}
    \label{fig:ed-vs-delta}
\end{figure}

\section{Discussion}

\subsection{Advantages}

\begin{itemize}
    \item \textbf{Structural awareness}: Graph edges encode relationships invisible to embedding similarity
    \item \textbf{Infinite scalability}: Navigate rather than load; context size bounded by query complexity, not corpus size
    \item \textbf{Interpretable retrieval}: Graph paths explain why context was included
    \item \textbf{Incremental updates}: Add/modify nodes without full reindexing
\end{itemize}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Graph construction cost}: Initial knowledge graph requires schema design and population
    \item \textbf{Schema rigidity}: Edge types must be predefined; emergent relationships require schema evolution
    \item \textbf{Query complexity}: Optimal $\alpha$ balancing may be query-dependent
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
    \item \textbf{Learned traversal policies}: Train agents to navigate manifolds adaptively
    \item \textbf{Cross-manifold linking}: Connect code, requirements, and documentation manifolds
    \item \textbf{Manifold-aware fine-tuning}: Train LLMs to consume manifold-structured context natively
\end{itemize}

\section{Conclusion}

The Context Manifold represents a fundamental shift from treating LLM context as flat sequences to modeling it as navigable topological structures. By combining knowledge graphs with embedded vector references, we enable AI systems to reason about arbitrarily complex domains without the information loss inherent in compression or truncation. Our economic analysis suggests substantial ROI for enterprise deployments, and we provide concrete experimental protocols for validation.

The reference implementations cv-git and cv-prd demonstrate practical applicability to code intelligence and requirements management. As AI systems tackle increasingly complex real-world tasks, the ability to maintain structural awareness across unlimited context will become not just advantageous but essential.

\section*{Acknowledgments}

The author thanks Ashraf Alyan (Foxconn) for the original conversation that inspired the Context Manifold concept and for valuable discussions on knowledge graph architectures for enterprise AI systems. Thanks also to Amir More (Baseshift) for the introduction to FalkorDB, which informed the graph database architecture presented in this work.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Yu et~al.(2023)Yu, Zhang, and Li]{codereval2023}
Yu, T., Zhang, R., and Li, J. (2023).
\newblock CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models.
\newblock \textit{arXiv preprint arXiv:2302.00288}.

\bibitem[Wu et~al.(2024)Wu, Guan, and Chen]{ragtruth2024}
Wu, Y., Guan, J., and Chen, Z. (2024).
\newblock RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models.
\newblock \textit{arXiv preprint arXiv:2401.00396}.

\bibitem[Liu et~al.(2024)Liu, Wang, and Zhang]{hallucode2024}
Liu, F., Wang, S., and Zhang, T. (2024).
\newblock Exploring and Evaluating Hallucinations in LLM-Powered Code Generation.
\newblock \textit{arXiv preprint arXiv:2404.00971}.

\bibitem[FalkorDB(2024)]{kglm2024}
FalkorDB (2024).
\newblock GraphRAG vs. Vector RAG Benchmark.
\newblock \url{https://www.falkordb.com/blog/graph-rag-vs-vector-rag/}.

\bibitem[Microsoft(2024)]{graphrag2024}
Microsoft Research (2024).
\newblock GraphRAG: Unlocking LLM discovery on narrative private data.
\newblock \url{https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/}.

\end{thebibliography}

\end{document}
